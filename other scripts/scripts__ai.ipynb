{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d495a3",
   "metadata": {},
   "source": [
    "# This code is generated by Grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, AzureOpenAIEmbeddings \n",
    "from langchain.prompts import PromptTemplate\n",
    "import fitz  # PyMuPDF for PDF handling\n",
    "from base64 import b64encode\n",
    "from typing import Literal, List, Dict, Tuple, NoReturn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f80b901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39241bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_azure_client() -> AzureOpenAI:\n",
    "    \"\"\" This function will load all the credentials and setup GPT-4o-mini LLM model\n",
    "        afterthat return as instance of Azure Open AI class\n",
    "\n",
    "    Returns:\n",
    "        AzureOpenAI: Instance of class (main execution) \n",
    "    \"\"\" \n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    deployment = \"gpt-4o-mini\"\n",
    "\n",
    "    client = AzureOpenAI()\n",
    "\n",
    "    return client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bdd51b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = load_azure_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13e060f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize embeddings for summaries\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ef674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663dc6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bacd9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1-2: Assume PDF is provided and images are extracted to images/ with doc_id\n",
    "def process_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num, page in enumerate(doc):\n",
    "        images = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            doc_id = f\"doc_{page_num}_{img_index}\"\n",
    "            with open(f\"images/{doc_id}.png\", \"wb\") as f:\n",
    "                f.write(base_image[\"image\"])\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1a5b0573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_pdf(\"content/attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "601acf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "online_transformer_image_path = \"https://media.geeksforgeeks.org/wp-content/uploads/20250529164119039854/Screenshot-2024-05-24-112357-768.webp\"\n",
    "\n",
    "# Step 4: Generate summaries for each image using GPT-4o-mini\n",
    "def generate_image_summary(image_path: str):\n",
    "\n",
    "    client = load_azure_client()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert vision assistant. Generate a summary for the given image only.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_path}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=4096,\n",
    "        temperature=1.0,\n",
    "        top_p=1.0\n",
    "    )\n",
    "\n",
    "    summary: str = response.choices[0].message.content                  # fetching summary from llm response\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f205d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_image_summary(online_transformer_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fdcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===  Images summary generation ===\n",
    "def generate_summary_of_each_image(\n",
    "        extracted_images_folder_path: str, \n",
    "        verbose: bool = True\n",
    "        \n",
    "        ) -> Tuple[AzureOpenAI.chat, Dict[str, str], Dict[str, str]]:\n",
    "    \n",
    "    \"\"\" This function takes the path of folder in which extracted images are stores,\n",
    "        and generate the summary of each image by the use of LLM, finally returned it.\n",
    "\n",
    "    Args:\n",
    "        extracted_images_folder_path (str): Path of folder that contains extracted images from PDF\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Returns a python dictionary that contains the unique doc_id as key as \n",
    "                        generated summary of images as value\n",
    "    \"\"\"\n",
    "\n",
    "    # checking provided folder path exists or not\n",
    "    if not os.path.exists(extracted_images_folder_path):\n",
    "        raise FileNotFoundError(f\"Folder doesn't exists: {extracted_images_folder_path}. Try to provide full folder path.\")\n",
    "\n",
    "    ## loading model\n",
    "    client = load_azure_client()\n",
    "\n",
    "    # generating embeddings of summaries\n",
    "    embed_summaries: Dict[str,str] = {}\n",
    "    generated_summary: Dict[str,str] = dict()                               # defining variable which store generated summaries of each image\n",
    "    images_path: List[str] = os.listdir(extracted_images_folder_path)       # fetching all images paths\n",
    "    total_images_count = len(images_path)                                   # extracting total number of images\n",
    "\n",
    "    ## iteration of each image\n",
    "    for index,image_path in enumerate(images_path):\n",
    "        doc_id = image_path.split('.')[0]                                   # extracting doc_id from image title\n",
    "        \n",
    "        # updating image path with its parent folder\n",
    "        image_full_path = os.path.join(extracted_images_folder_path, image_path)\n",
    "                               \n",
    "        # fetching image data\n",
    "        with open(image_full_path, \"rb\") as f:\n",
    "            image_b64 = b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Generating summary of image ({index+1}/{total_images_count}).... \")\n",
    "\n",
    "\n",
    "        data_url = f\"data:image/png;base64,{image_b64}\"                     # creating data url variable\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert vision assistant.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Generate a detailed summary for the given image only. Making sure you summarize everything what you see in the image.\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=4096,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0\n",
    "        )\n",
    "\n",
    "        summary: str = response.choices[0].message.content                  # fetching summary from llm response\n",
    "        summary_embedding = embedding_model.embed_query(summary)            # getting embedding of generated summary\n",
    "        generated_summary.update({doc_id:summary})                          # add generated summary to summaries\n",
    "        embed_summaries.update({doc_id:summary_embedding})                # add summary embedding to summary embeddings\n",
    "\n",
    "        \n",
    "\n",
    "    return response, generated_summary, embed_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d09c5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,summaries, embed_summaries = generate_summary_of_each_image('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3e2d293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2c100eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5-6: Generate embeddings and save to FAISS\n",
    "def save_to_vector_store():\n",
    "    \n",
    "    # getting summaries\n",
    "    _,_,embed_summaries = generate_summary_of_each_image(\"images\")\n",
    "\n",
    "    for key, embed_summary in embed_summaries.items():\n",
    "        doc_id = key\n",
    "        vector_store = FAISS.from_embeddings(\n",
    "            [(doc_id, embed_summary)],\n",
    "            embedding=embedding_model\n",
    "        )\n",
    "\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4fe7efe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary of image (1/3).... \n",
      "Generating summary of image (2/3).... \n",
      "Generating summary of image (3/3).... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_to_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25065a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "79c2f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 7-9: Handle user query and display response\n",
    "def query_system(user_query):\n",
    "    vector_store = FAISS.load_local(\n",
    "        folder_path = \"faiss_index\", \n",
    "        embeddings = embedding_model,\n",
    "        allow_dangerous_deserialization = True\n",
    "    )\n",
    "    # results = vector_store.similarity_search(user_query, k=1)\n",
    "\n",
    "    # creating retriever\n",
    "    retriver = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={'score_threshold': 0.6}\n",
    "    )\n",
    "\n",
    "    results = retriver.invoke(user_query)\n",
    "\n",
    "    if not results:\n",
    "        print(\"I don't know (No image applicable for you this query)\")\n",
    "        return\n",
    "    \n",
    "    doc_id = results[0].page_content\n",
    "    image_path = f\"images/{doc_id}.png\"\n",
    "\n",
    "    # fetching image data\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_b64 = b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    data_url = f\"data:image/png;base64,{image_b64}\"  \n",
    "    \n",
    "    # Step 8: Pass image to LLM with prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert vision assistant. Don't use phrases like 'In the context of image...' or its relevant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": f\"Answer the query in the context of provided image. Query: {user_query}\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=4096,\n",
    "        temperature=1.0,\n",
    "        top_p=1.0\n",
    "    )\n",
    "\n",
    "    # Step 9: Display response\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V (values) and K (keys) are two distinct components in the attention mechanism of neural networks. The primary difference lies in their roles within the attention process. \\n\\n1. **Purpose**: \\n   - K (keys) represents information that the model uses to determine relevance when computing attention scores. It serves as the criteria against which queries will be matched.\\n   - V (values), on the other hand, contains the actual information to be attended to or the output that will be produced based on the attention weights calculated from the queries and keys.\\n\\n2. **Computation**:\\n   - The attention mechanism calculates similarity scores between queries (Q) and keys (K). These scores dictate how much emphasis should be placed on each value (V).\\n   - After computing the attention scores, these scores are used to weight the values (V), determining how much information from V is passed on to subsequent layers.\\n\\nIn summary, while K is used to assess the relevance and scoring through queries, V holds the actual content that is retrieved and influenced by those scores.'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = query_system(\"why v is different than k\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1afb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75428d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists('images')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
