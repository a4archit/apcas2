{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd89c05a",
   "metadata": {},
   "source": [
    "# Python Scripts testings and development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371abbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/archit-elitebook/workarea/whole working/genai/projects/APCAS2/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import open_clip # open_clip_torch\n",
    "from PIL import Image\n",
    "import faiss\n",
    "from openai import AzureOpenAI\n",
    "import numpy as np\n",
    "from base64 import b64encode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92d2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ SETUP\n",
    "# -------------------------------\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "# Load open_clip model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f8b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ IMAGE EMBEDDING & INDEX\n",
    "# -------------------------------\n",
    "\n",
    "image_folder = \"images/\"\n",
    "image_paths = glob.glob(os.path.join(image_folder, \"*.png\"))\n",
    "\n",
    "image_embeddings = []\n",
    "image_ids = []\n",
    "\n",
    "for img_path in image_paths:\n",
    "    image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_emb = model.encode_image(image)\n",
    "        img_emb /= img_emb.norm(dim=-1, keepdim=True)\n",
    "        img_emb = img_emb.cpu().numpy()\n",
    "    image_embeddings.append(img_emb)\n",
    "    image_ids.append(img_path)\n",
    "\n",
    "image_embeddings = np.vstack(image_embeddings).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatIP(image_embeddings.shape[1])\n",
    "index.add(image_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44de4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_question = \"how many feed forward layers in the given image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44ec5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ TEXT EMBEDDING\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "text_tokens = tokenizer([user_question]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_emb = model.encode_text(text_tokens)\n",
    "    text_emb /= text_emb.norm(dim=-1, keepdim=True)\n",
    "    text_emb = text_emb.cpu().numpy().astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0c3d64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matches: ['images/09a9f4be-8057-43d7-bdf2-a20404ab2165.png']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ SEARCH\n",
    "# -------------------------------\n",
    "\n",
    "k = 1\n",
    "D, I = index.search(text_emb, k)\n",
    "top_image_paths = [image_ids[i] for i in I[0]]\n",
    "\n",
    "print(\"Top matches:\", top_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaaf299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a450a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° LLM Response:\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ AZURE GPT-4o-mini\n",
    "# -------------------------------\n",
    "\n",
    "context = f\"The user asked: '{user_question}'. Relevant images: {', '.join(top_image_paths)}.\"\n",
    "\n",
    "img_path = top_image_paths[0] # \"images/07393f28-9525-4a42-9bb5-2d153696739e.png\"\n",
    "\n",
    "# fetching image data\n",
    "with open(img_path, \"rb\") as f:\n",
    "    image_b64 = b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "data_url = f\"data:image/png;base64,{image_b64}\"                     # creating data url variable\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert vision assistant. Answer the query from the given image ONLY, if no answer will found then say you don't know.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": f\"user query: {user_question}\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "print(\"\\nüí° LLM Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b9478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9cf63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
